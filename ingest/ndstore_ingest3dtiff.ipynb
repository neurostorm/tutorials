{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting 3D Data Volumes With Annotations in HDF5 Format to ndstore\n",
    "\n",
    "In this tutorial we will show you how to take a 3D image volume and 3D annotation volume and: create a dataset, project, tokens and channels, ingest the data, and verify the process was successful.\n",
    "\n",
    "\n",
    "### Step 1: Prepare your data\n",
    "\n",
    "First ensure that your data is written to a properly formatted hdf5 volume. As an example, [this link](http://openconnecto.me/ocp/ca/kasthuri11/image/hdf5/3/1000,1300/2000,2200/1000,1200/) provides a 300x200x200 tiff image of electron microscopy data, and [this link](http://openconnecto.me/ocp/ca/kat11segments/annotation/hdf5/3/1000,1300/2000,2200/1000,1200/) provides annotations for the same region.\n",
    "\n",
    "### Step 2: Interogating your data\n",
    "\n",
    "In order to create a dataset, project, and channels for your data, you first need to understand several details of your data. Some of them can be found by interogating the images, while others require insight into the data acquisition (such as resolution, for instance).\n",
    "\n",
    "The details which can be determined from your image are:\n",
    "  - {x, y, z} image size\n",
    "  - time range\n",
    "  - data type\n",
    "  - window range\n",
    "\n",
    "The details which require information about your particular data are:\n",
    "  - dataset name (no spaces or special characters)\n",
    "  - {x, y, z} offset\n",
    "  - scaling levels\n",
    "  - scaling option\n",
    "  - {x, y, z} voxel resolution\n",
    "\n",
    "A description of each of these fields is available [here](http://docs.neurodata.io/ndstore/sphinx/datamodel.html#dataset-attributes).\n",
    "\n",
    "In order to determine the details of interest from our data in HDF5 format, we will use Python's `h5py` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X image size:  200\n",
      "Y image size:  200\n",
      "Z image size:  300\n",
      "Time series:  False\n",
      "Time range: (0, 0)\n",
      "Data type:  uint8\n",
      "Window range: (0.000000, 254.000000)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "filename = 'kasthuri11-image-hdf5-3-1000_1300-2000_2200-1000_1200-ocpcutout.h5'\n",
    "with h5py.File(filename,'r') as hf:\n",
    "    data = hf.get(hf.keys()[0])\n",
    "    np_data = np.array(data.get('CUTOUT'))\n",
    "\n",
    "print 'X image size: ', np_data.shape[0]\n",
    "print 'Y image size: ', np_data.shape[1]\n",
    "print 'Z image size: ', np_data.shape[2]\n",
    "print 'Time series: ', (len(np_data.shape) == 4)\n",
    "if (len(np_data.shape) == 4):\n",
    "    print 'Time range: (%d, %d)' % (0, np.shape[3]-1)\n",
    "else:\n",
    "    print 'Time range: (0, 0)'\n",
    "print 'Data type: ', np_data.dtype\n",
    "print 'Window range: (%f, %f)' % (np.min(np_data), np.max(np_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing these results and those that must be determined with more intimate knowledge of the data, we come up with the following list:\n",
    "\n",
    "  - dataset name: neurostorm_kat11_scale3\n",
    "  - x size: 200\n",
    "  - y size: 200\n",
    "  - z size: 300\n",
    "  - time range: (0, 0)\n",
    "  - data type: uint8\n",
    "  - window range: (0, 254)\n",
    "  - x offset: 1000\n",
    "  - y offset: 2000\n",
    "  - z offset: 1000\n",
    "  - scaling levels: 0\n",
    "  - scaling option: z-slices\n",
    "  - x voxel resolution: 32 nm\n",
    "  - y voxel resolution: 32 nm\n",
    "  - z voxel resolution: 40 nm\n",
    "\n",
    "### Step 3:  Create A Dataset\n",
    "\n",
    "Once you have this metadata about your images, you can begin the process of creating a dataset.\n",
    "\n",
    "First, you should navigate to the server you wish to use (http://openconnecto.me/ocp/accounts/login/, for instance) and login (or register if you don't have an account). Once you enter your login information, you will be able to select the `Datasets > Create New Dataset` menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
